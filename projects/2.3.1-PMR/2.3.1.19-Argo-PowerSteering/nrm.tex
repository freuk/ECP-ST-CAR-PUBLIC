\subsubsection*{NRM}

\paragraph{Overview}

Argo Node Resource Manager (NRM) is a daemon running on the compute
nodes. It centralizes node management activities such as job management,
resource management, and power management.

NRM interacts with both global resource management services (e.g., job
scheduler) and with application components and runtime services running on
the node. It acts as a control infrastructure to enable custom resource
management policies at the node level.  Applications can influence those
mechanisms, both directly (through explicit API calls used, e.g., to
request additional resources on the node) and indirectly (by having their
run-time behavior monitored by NRM).

\paragraph{Key Challenges}

Many ECP applications have a complex runtime structure, ranging from in
situ data analysis, through an ensemble of largely independent individual
subjobs, to arbitrarily complex workflow structures.  At the same time, HPC
hardware complexity increases as well, from deeper memory hierarchies to
heterogeneous compute resources and performance fluctuating based on
power/thermal constraints.

Even in the common case of each compute node being allocated exclusively to
a single job, managing available node resources can be a challenge.  If a
compute node is shared among multiple job components (a likely scenario
considering the reduced cost of data transfers), these components---if
allowed to freely share node resources---could interfere with one another,
resulting in suboptimal performance.  It is the NRM's job to rein in this
complexity by acting as a coarse-grained resource arbitrator.

\paragraph{Solution Strategy}

% NRM uses \emph{slices} for resource management.  Physical resources on the
% compute nodes are divided into separate partitions.  Slices can be used to
% separate individual components of parallel workloads, in addition to
% cordoning off system services; this physical separation ensures an improved
% performance isolation between components.

NRM uses an active approach to resource management. Physical and logical
resources on the compute nodes are configured, discovered, and accounted for.
NRM can manage compute nodes and individual components of parallel workloads,
in the sense that it performs an active accounting of two types of interfaces
for those workloads: sensors, and actuators. Those abstracted components are
available through an upstream API which enables their discovery and management.

The NRM daemon supports power actuators, and sensors based on CPU counters,
and power information. We also provide a simple API that application processes
can use to periodically update the NRM on their progress. This gives NRM
reliable feedback on the efficacy of its power policies, and it can also
be used for a more robust identification of the critical path, rather than
relying on heuristics based on performance counters.

\begin{wrapfigure}[11]{r}{.58\textwidth}
  \includegraphics[width=.58\textwidth]{projects/2.3.1-PMR/2.3.1.19-Argo-PowerSteering/sensors}
\end{wrapfigure}
In addition to those capabilities, NRM's configuration format allows for
the configuration of actuators and active polling sensors based on arbitrary
executables. The figure on the right shows a reading of NRM's sensor
outputs for two RAPL sensors along with CPU counter instrumentation. In order
to provide those capabilities, the NRM daemon manages the launching, management
and stopping of parallel workloads through an unified interface. Resources
can be dynamically reconfigured at run time; those interfaces are provided
for use from applications and from global services.

\begin{wrapfigure}[10]{l}{.38\textwidth}
\includegraphics[width=.38\textwidth]{projects/2.3.1-PMR/2.3.1.19-Argo-PowerSteering/cpd}
\end{wrapfigure}
Other than sensor and actuator accounting capabilities, NRM provides
optional support for autonomic node management. This support is provided through
the accounting of node-local autonomic goals and constraints, which are
expressed in terms of available nodes and sensors. Those goals and
constraints are inspectable by the users, the global services (GRM), and
applications through NRM's unified interface. Inspection is achieved through
the emission of a Control Description Format(CPD), which outlines active
sensors, actuators, goals and constraints. This scheme is outlined by the
figure on the left.

\paragraph{Recent Progress}

%We developed an API between Node Power and Node Resource Manager (NRM),
%which in turn allows Global Resource Manager (GRM) to control and monitor
%power and other node-local resources.  Additionally, we studied the effect
%of power capping on different applications using the NodePower API and
%developed power regression models required for a demand-response policy.

%We developed the first version of the unified Node Resource Manager.  The
%NRM provides high level of control over node resources, including initial
%allocation at job launch and dynamic reallocation at the request of the
%application and other services.  The initial set of managed resources
%includes CPU cores and memory; they can be allocated to application
%components via a container abstraction, which is used to describe
%partitions of physical resources (to decrease interference), and more.  NRM
%integrates dynamic power control using COOLR and libMSR, and provides
%support for tracking and reporting of application progress.  Such
%functionality is needed by the BSP-oriented power policy, which we are
%currently evaluating and scaling up.  Work is also ongoing to support
%third-party container technologies such as Docker, Singularity, and
%Shifter.  At the job level, we enabled enclave-aware MPI facilities that
%can be used to create inter-communicators between MPI jobs launched in
%separate enclaves.

The NRM daemon has undergone major improvements. In order to improve the
software quality and overall reliability of NRM, the core business logic
of the NRM daemon has been externalized in a shared library, written in the
statically typed language Haskell. This NRM version is pending an upcoming
release. Our custom CI pipeline also has been improved, with the addition
of unit tests and enrichment of existing functional tests. We are still in
the process of leveraging ECP testing infrastructure.

NRM has recently added support for multi-armed bandit based control policies.
In order to make this work as robust as possible, this work has been
compartmentalized into a software library (\textit{hbandit}). \textit{hbandit}
implements statically typed, independently tested versions of the relevant
machine learning algorithms.

NRM now offers a Python support library for upstream control. This library is
intended for workload-specific control algorithm experiments.

We continue supporting the \texttt{libnrm} C library that can be linked to
applications in order to provide reports on application progress to NRM. We have
instrumentation for EXAALT, QMCPACK, ExaSMR, AMG, Stream, and CANDLE.
This capability gives us insight into the effect of our resource management
policies on the run-time behavior of user codes.

% \begin{figure}
% \centering
% \includegraphics[width=.45\textwidth]{projects/2.3.1-PMR/2.3.1.19-Argo-PowerSteering/nrm-runtime}
% \includegraphics[width=.45\textwidth]{projects/2.3.1-PMR/2.3.1.19-Argo-PowerSteering/nrm-energy}
% \caption{Cumulative run time and energy for different applications and
%           benchmarks running with a varying CPU power cap.}
% \label{fig:argo:nrm-results}
% \end{figure}

\paragraph{Next Steps}

We are working to add support for workload-specific control-theoretic
resource management to NRM. This work is a collaboration with an external
team, supported by our python upstream interface.

We are working on a report to showcase the use of NRM's current autonomic
resource management strategies in the context of workload energy minimization,
through the use of RAPL control and application feedback.

We are working on expanding the set of ECP applications that are instrumented
to report their progress to NRM.

We are planning to expand the list of resources managed by NRM by
adding support for the partitioning of CPU caches and other vendor-specific
mechanisms.
